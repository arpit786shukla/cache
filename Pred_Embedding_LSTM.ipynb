{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict \n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import keras \n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Input, Dropout, Embedding, LSTM, Flatten, Lambda, GRU\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDR = '/nfs_home/nbhardwaj/data/rds_final/'\n",
    "w_ADDR = '/nfs_home/nbhardwaj/model_weights/LSTM_wts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        m = {}\n",
    "        for x in self.label_encoder.classes_:\n",
    "            m[x] = True\n",
    "        for ind, y in enumerate(new_data_list):\n",
    "            if(m.get(y) is None):\n",
    "                new_data_list[ind] = 'Unknown'\n",
    "#         for unique_item in np.unique(data_list):\n",
    "#             if unique_item not in self.label_encoder.classes_:\n",
    "#                 new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "        return self.label_encoder.transform(new_data_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embed_size = 10, window_size = 3):\n",
    "    inp1 = Input(shape = (window_size,))\n",
    "    inp2 = Input(shape = (window_size,))\n",
    "\n",
    "    embed1 = Embedding(len(le_inst.classes_), embed_size, input_length = window_size)(inp1)\n",
    "    embed2 = Embedding(len(le_delta.classes_), embed_size, input_length = window_size)(inp2)\n",
    "\n",
    "    merged_inp = keras.layers.concatenate([embed1, embed2], axis = 1)\n",
    "    # merged_inp = Flatten()(merged_inp)\n",
    "    # # merged_inp = keras.layers.concatenate([merged_inp, inp3])\n",
    "    \n",
    "    out = GRU(64)(merged_inp)\n",
    "    out = Dense(32, activation = 'relu')(out)\n",
    "    out = Dense(8, activation = 'softmax')(out)\n",
    "\n",
    "    model = Model([inp1, inp2], out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61dcc45336ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m          ]\n\u001b[1;32m      6\u001b[0m \u001b[0msets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'fname'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'len'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inst_v'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'delt_v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "embed_size = 10\n",
    "files = [510\n",
    "          , 511, 526, 600, 602, 620, 623, 625, 631, 641, 648, 657\n",
    "         ]\n",
    "sets = [x for x in range(64)]\n",
    "df_m = pd.DataFrame(columns = ['fname', 'set', 'train_acc', 'test_acc', 'len', 'inst_v', 'delt_v'])\n",
    "for fname in files:\n",
    "    begin = time.time()\n",
    "    cw_ADDR = w_ADDR+str(fname)+'/'\n",
    "    for cset in sets:\n",
    "        if(not os.path.isdir(cw_ADDR)):\n",
    "            os.system(\"mkdir \"+cw_ADDR)\n",
    "            \n",
    "        df = pd.read_csv(ADDR+str(fname)+'_'+str(cset)+'.csv', index_col = [0], usecols = [0,2,7,8])\n",
    "        df.Instruction = df.Instruction.astype('str')\n",
    "        df.delta = df.delta.astype('float')\n",
    "\n",
    "        X = df[['Instruction', 'delta']].values[1:]\n",
    "        y = df[['label']].values[1:]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "        print(\"--------------------split done---------------------\")\n",
    "        le_inst = LabelEncoderExt()\n",
    "        le_inst.fit(X_train[:, 0])\n",
    "        le_delta = LabelEncoderExt()\n",
    "        le_delta.fit(X_train[:, 1])\n",
    "        print(\"----------------labels done----------------------\")\n",
    "        X_train[:, 0] = le_inst.transform(X_train[:, 0])\n",
    "        X_train[:, 1] = le_delta.transform(X_train[:, 1])\n",
    "        print(\"--------\")\n",
    "\n",
    "        X_test[:, 0] = le_inst.transform(X_test[:, 0])\n",
    "        X_test[:, 1] = le_delta.transform(X_test[:, 1])\n",
    "        print(\"-------------------labels transformed---------------------\")\n",
    "        y_train = to_categorical(y_train)\n",
    "        y_test = to_categorical(y_test)\n",
    "        X_train, y_train = TimeseriesGenerator(X_train, y_train, window_size, batch_size = len(X_train))[0]\n",
    "        X_test, y_test = TimeseriesGenerator(X_test, y_test, window_size, batch_size = len(X_test))[0]\n",
    "        print(\"---------window shit done-----------\")\n",
    "        filepath = cw_ADDR+str(fname)+'_'+str(cset)+'-gru.hdf5'\n",
    "\n",
    "        if(os.path.isfile(filepath)):\n",
    "            model = load_model(filepath)\n",
    "            print(\"using loaded model\", filepath)\n",
    "        else:\n",
    "            model = create_model()\n",
    "            model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "#         model = create_model(embed_size, window_size)\n",
    "#         model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "#         print(model.summary())\n",
    "        es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 2)\n",
    "        mc = ModelCheckpoint(filepath, monitor = 'val_accuracy', save_best_only = True, mode = 'max')\n",
    "        history = model.fit([X_train[:, :, 0], X_train[:, :, 1]] , y_train, epochs = 50, \n",
    "                  validation_split = 0.2, use_multiprocessing = True, verbose = 2, callbacks = [es, mc], batch_size = 8192)\n",
    "\n",
    "        print(\"------------training done------------\")\n",
    "#         model.save_weights(filepath)\n",
    "        t_ac = model.evaluate([X_test[:,:, 0], X_test[:,:, 1]], (y_test), verbose = 2)[1]\n",
    "        tr_ac = model.evaluate([X_train[:,:, 0], X_train[:,:, 1]], (y_train), verbose = 2)[1]\n",
    "        \n",
    "        # need to reduce len to test_size later\n",
    "        df_m.loc[len(df_m)] = [fname, cset, tr_ac, t_ac, len(df), len(le_inst.classes_), len(le_delta.classes_)]\n",
    "\n",
    "        print(\"--------------done processing for set---------->\", cset, '|| accuracy||', t_ac)\n",
    "        print( '|| accuracy||', tr_ac, t_ac)\n",
    "        print(\"inst vocal\", len(le_inst.classes_), \"|| delta vocal->\", len(le_delta.classes_))\n",
    "        print(\"____________________________________________________________________________________________________\")\n",
    "    df_m.to_csv(cw_ADDR+'results.csv')\n",
    "    \n",
    "    print(\"$$$$$$$ done for file ||------------>\", fname, \"|| \", time.time()-begin, \"seconds\")\n",
    "df_m.to_csv(w_ADDR+'results.csv')\n",
    "print(\"---------ALL DONE___________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "---------window shit done-----------\n",
      "using loaded model /nfs_home/nbhardwaj/model_weights/LSTM_wts/510/510_0-gru.hdf5\n",
      "------------training done------------\n",
      "--------------done processing for set----------> 0 || accuracy|| 0.864420473575592\n",
      "|| accuracy|| 0.8646900653839111 0.864420473575592\n",
      "inst vocal 212 || delta vocal-> 28897\n",
      "____________________________________________________________________________________________________\n",
      "$$$$$$$ done for file ||------------> 510 ||  115.24152183532715 seconds\n",
      "---------ALL DONE___________\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "embed_size = 10\n",
    "files = [510\n",
    "#           , 511, 526, 600, 602, 620, 623, 625, 631, 641, 648, 657\n",
    "         ]\n",
    "sets = [0\n",
    "#     x for x in range(64)\n",
    "]\n",
    "df_m = pd.DataFrame(columns = ['fname', 'set', 'train_acc', 'test_acc', 'len', 'inst_v', 'delt_v'])\n",
    "for fname in files:\n",
    "    begin = time.time()\n",
    "    cw_ADDR = w_ADDR+str(fname)+'/'\n",
    "    for cset in sets:\n",
    "#         if(not os.path.isdir(cw_ADDR)):\n",
    "#             os.system(\"mkdir \"+cw_ADDR)\n",
    "            \n",
    "        df = pd.read_csv(ADDR+str(fname)+'_'+str(cset)+'.csv', index_col = [0], usecols = [0,2,7,8])\n",
    "        df.Instruction = df.Instruction.astype('str')\n",
    "        df.delta = df.delta.astype('float')\n",
    "\n",
    "        X = df[['Instruction', 'delta']].values[1:]\n",
    "        y = df[['label']].values[1:]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "        print(\"--------------------split done---------------------\")\n",
    "        le_inst = LabelEncoderExt()\n",
    "        le_inst.fit(X_train[:, 0])\n",
    "        le_delta = LabelEncoderExt()\n",
    "        le_delta.fit(X_train[:, 1])\n",
    "        print(\"----------------labels done----------------------\")\n",
    "        X_train[:, 0] = le_inst.transform(X_train[:, 0])\n",
    "        X_train[:, 1] = le_delta.transform(X_train[:, 1])\n",
    "        print(\"--------\")\n",
    "\n",
    "        X_test[:, 0] = le_inst.transform(X_test[:, 0])\n",
    "        X_test[:, 1] = le_delta.transform(X_test[:, 1])\n",
    "        print(\"-------------------labels transformed---------------------\")\n",
    "        y_train = to_categorical(y_train)\n",
    "        y_test = to_categorical(y_test)\n",
    "        X_train, y_train = TimeseriesGenerator(X_train, y_train, window_size, batch_size = len(X_train))[0]\n",
    "        X_test, y_test = TimeseriesGenerator(X_test, y_test, window_size, batch_size = len(X_test))[0]\n",
    "        print(\"---------window shit done-----------\")\n",
    "        filepath = cw_ADDR+str(fname)+'_'+str(cset)+'-gru.hdf5'\n",
    "\n",
    "        if(os.path.isfile(filepath)):\n",
    "            model = load_model(filepath)\n",
    "            print(\"using loaded model\", filepath)\n",
    "        else:\n",
    "            model = create_model()\n",
    "            model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "#         model = create_model(embed_size, window_size)\n",
    "#         model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "#         print(model.summary())\n",
    "#         es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 2)\n",
    "#         mc = ModelCheckpoint(filepath, monitor = 'val_accuracy', save_best_only = True, mode = 'max')\n",
    "#         history = model.fit([X_train[:, :, 0], X_train[:, :, 1]] , y_train, epochs = 50, \n",
    "#                   validation_split = 0.2, use_multiprocessing = True, verbose = 2, callbacks = [es, mc], batch_size = 8192)\n",
    "\n",
    "        print(\"------------training done------------\")\n",
    "#         model.save_weights(filepath)\n",
    "        t_ac = model.evaluate([X_test[:,:, 0], X_test[:,:, 1]], (y_test), verbose = 2, use_multiprocessing = True, batch_size=4096)[1]\n",
    "        tr_ac = model.evaluate([X_train[:,:, 0], X_train[:,:, 1]], (y_train), verbose = 2, use_multiprocessing = True, batch_size = 4096)[1]\n",
    "        \n",
    "        # need to reduce len to test_size later\n",
    "#         df_m.loc[len(df_m)] = [fname, cset, tr_ac, t_ac, len(df), len(le_inst.classes_), len(le_delta.classes_)]\n",
    "\n",
    "        print(\"--------------done processing for set---------->\", cset, '|| accuracy||', t_ac)\n",
    "        print( '|| accuracy ||', tr_ac, t_ac)\n",
    "        print(\"inst vocal\", len(le_inst.classes_), \"|| delta vocal->\", len(le_delta.classes_))\n",
    "        print(\"____________________________________________________________________________________________________\")\n",
    "#     df_m.to_csv(cw_ADDR+'results.csv')\n",
    "    \n",
    "    print(\"$$$$$$$ done for file ||------------>\", fname, \"|| \", time.time()-begin, \"seconds\")\n",
    "# df_m.to_csv(w_ADDR+'results.csv')\n",
    "print(\"---------ALL DONE___________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
