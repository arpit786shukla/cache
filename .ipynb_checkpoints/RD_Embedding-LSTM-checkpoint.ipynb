{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict \n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Embedding, LSTM, Flatten, Lambda\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDR = '/nfs_home/nbhardwaj/data/rds_data/400/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        m = {}\n",
    "        for x in self.label_encoder.classes_:\n",
    "            m[x] = True\n",
    "        for ind, y in enumerate(new_data_list):\n",
    "            if(m.get(y) is None):\n",
    "                new_data_list[ind] = 'Unknown'\n",
    "#         for unique_item in np.unique(data_list):\n",
    "#             if unique_item not in self.label_encoder.classes_:\n",
    "#                 new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "        return self.label_encoder.transform(new_data_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "for x in a:\n",
    "    if(x ==1):\n",
    "        x=0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embed_size = 10):\n",
    "    inp1 = Input(shape = (1,))\n",
    "    inp2 = Input(shape = (1,))\n",
    "    inp3 = Input(shape = (1,))\n",
    "\n",
    "    embed1 = Embedding(len(le_inst.classes_), embed_size, input_length = 1)(inp1)\n",
    "    embed2 = Embedding(len(le_delta.classes_), embed_size, input_length = 1)(inp2)\n",
    "\n",
    "    merged_inp = keras.layers.concatenate([embed1, embed2], axis = 1)\n",
    "    merged_inp = Flatten()(merged_inp)\n",
    "    merged_inp = keras.layers.concatenate([merged_inp, inp3])\n",
    "    \n",
    "    out = LSTM(64)(merged_inp)\n",
    "    out = Dense(32, activation = 'relu')(merged_inp)\n",
    "    out = Dense(8, activation = 'softmax')(out)\n",
    "\n",
    "    model = Model([inp1, inp2, inp3], out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "178847/178847 [==============================] - 15s 82us/step - loss: 0.3725 - accuracy: 0.8805\n",
      "Epoch 2/2\n",
      "178847/178847 [==============================] - 15s 81us/step - loss: 0.3282 - accuracy: 0.8918\n",
      "------------training done------------\n",
      "88089/88089 [==============================] - 3s 30us/step\n",
      "178847/178847 [==============================] - 6s 31us/step\n",
      "--------------done processing for set----------> 0 || accuracy|| 0.8951742053031921\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "112828/112828 [==============================] - 11s 97us/step - loss: 0.5696 - accuracy: 0.8187\n",
      "Epoch 2/2\n",
      "112828/112828 [==============================] - 11s 95us/step - loss: 0.4977 - accuracy: 0.8362\n",
      "------------training done------------\n",
      "55572/55572 [==============================] - 2s 35us/step\n",
      "112828/112828 [==============================] - 4s 34us/step\n",
      "--------------done processing for set----------> 1 || accuracy|| 0.8384438157081604\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "56527/56527 [==============================] - 5s 86us/step - loss: 0.5398 - accuracy: 0.8390\n",
      "Epoch 2/2\n",
      "56527/56527 [==============================] - 5s 82us/step - loss: 0.4033 - accuracy: 0.8710\n",
      "------------training done------------\n",
      "27842/27842 [==============================] - 1s 38us/step\n",
      "56527/56527 [==============================] - 2s 32us/step\n",
      "--------------done processing for set----------> 2 || accuracy|| 0.8696214556694031\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "163569/163569 [==============================] - 19s 118us/step - loss: 0.4034 - accuracy: 0.9000\n",
      "Epoch 2/2\n",
      "163569/163569 [==============================] - 20s 120us/step - loss: 0.3471 - accuracy: 0.9120\n",
      "------------training done------------\n",
      "80565/80565 [==============================] - 3s 37us/step\n",
      "163569/163569 [==============================] - 6s 36us/step\n",
      "--------------done processing for set----------> 3 || accuracy|| 0.9125550985336304\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "191835/191835 [==============================] - 16s 81us/step - loss: 0.4441 - accuracy: 0.8614\n",
      "Epoch 2/2\n",
      "191835/191835 [==============================] - 15s 78us/step - loss: 0.4027 - accuracy: 0.8705\n",
      "------------training done------------\n",
      "94486/94486 [==============================] - 4s 38us/step\n",
      "191835/191835 [==============================] - 7s 37us/step\n",
      "--------------done processing for set----------> 4 || accuracy|| 0.8719069361686707\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "348635/348635 [==============================] - 34s 97us/step - loss: 0.2842 - accuracy: 0.8984\n",
      "Epoch 2/2\n",
      "348635/348635 [==============================] - 33s 95us/step - loss: 0.2553 - accuracy: 0.9063\n",
      "------------training done------------\n",
      "171716/171716 [==============================] - 7s 38us/step\n",
      "348635/348635 [==============================] - 13s 37us/step\n",
      "--------------done processing for set----------> 5 || accuracy|| 0.9072480201721191\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "548735/548735 [==============================] - 45s 82us/step - loss: 0.5223 - accuracy: 0.7598\n",
      "Epoch 2/2\n",
      "548735/548735 [==============================] - 45s 82us/step - loss: 0.5030 - accuracy: 0.7650\n",
      "------------training done------------\n",
      "270273/270273 [==============================] - 10s 37us/step\n",
      "548735/548735 [==============================] - 21s 39us/step\n",
      "--------------done processing for set----------> 6 || accuracy|| 0.765836775302887\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "2109920/2245160 [===========================>..] - ETA: 11s - loss: 0.1418 - accuracy: 0.9590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2245160/2245160 [==============================] - 87s 39us/step\n",
      "--------------done processing for set----------> 7 || accuracy|| 0.9597495198249817\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      " 445376/3525763 [==>...........................] - ETA: 10:38 - loss: 0.3458 - accuracy: 0.9147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1690272/3525763 [=============>................] - ETA: 6:16 - loss: 0.3355 - accuracy: 0.9167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2811168/3525763 [======================>.......] - ETA: 2:29 - loss: 0.3338 - accuracy: 0.9170"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3525763/3525763 [==============================] - 752s 213us/step - loss: 0.3331 - accuracy: 0.9171\n",
      "Epoch 2/2\n",
      "3525763/3525763 [==============================] - 766s 217us/step - loss: 0.3304 - accuracy: 0.9175\n",
      "------------training done------------\n",
      "1736571/1736571 [==============================] - 68s 39us/step\n",
      "3525763/3525763 [==============================] - 138s 39us/step\n",
      "--------------done processing for set----------> 8 || accuracy|| 0.9180603623390198\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "4406800/4406800 [==============================] - 387s 88us/step - loss: 0.0480 - accuracy: 0.9872\n",
      "Epoch 2/2\n",
      "4406800/4406800 [==============================] - 384s 87us/step - loss: 0.0465 - accuracy: 0.9876\n",
      "------------training done------------\n",
      "2170514/2170514 [==============================] - 87s 40us/step\n",
      "4406800/4406800 [==============================] - 177s 40us/step\n",
      "--------------done processing for set----------> 9 || accuracy|| 0.9875941872596741\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "3069065/3069065 [==============================] - 382s 125us/step - loss: 0.1410 - accuracy: 0.9652\n",
      "Epoch 2/2\n",
      "3069065/3069065 [==============================] - 384s 125us/step - loss: 0.1387 - accuracy: 0.9656\n",
      "------------training done------------\n",
      "1511630/1511630 [==============================] - 62s 41us/step\n",
      "3069065/3069065 [==============================] - 127s 42us/step\n",
      "--------------done processing for set----------> 10 || accuracy|| 0.9657191038131714\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "1180623/1180623 [==============================] - 103s 87us/step - loss: 0.1317 - accuracy: 0.9624\n",
      "Epoch 2/2\n",
      "1180623/1180623 [==============================] - 102s 87us/step - loss: 0.1246 - accuracy: 0.9640\n",
      "------------training done------------\n",
      "581501/581501 [==============================] - 24s 42us/step\n",
      "1180623/1180623 [==============================] - 49s 42us/step\n",
      "--------------done processing for set----------> 11 || accuracy|| 0.9642632007598877\n",
      "--------------------split done---------------------\n",
      "----------------labels done----------------------\n",
      "--------\n",
      "-------------------labels transformed---------------------\n",
      "Epoch 1/2\n",
      "3488512/5269662 [==================>...........] - ETA: 3:44 - loss: 0.4127 - accuracy: 0.7274"
     ]
    }
   ],
   "source": [
    "sets = [x for x in range(64)]\n",
    "inst_vocab = []\n",
    "delta_vocab = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for cset in sets:\n",
    "    df = pd.read_csv(ADDR+str(cset)+'.csv', index_col = [0])\n",
    "    df.Mode = np.where(df.Mode.values=='R', 1, -1)\n",
    "    df.Mode = df['Mode'].astype('str')\n",
    "    df.Instruction = df.Instruction.astype('str')\n",
    "    df.delta = df.delta.astype('float')\n",
    "    df.label = df.label - 1\n",
    "    X = df[['Instruction', 'delta', 'Mode']].values[1:]\n",
    "    y = df[['label']].values[1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "    print(\"--------------------split done---------------------\")\n",
    "    le_inst = LabelEncoderExt()\n",
    "    le_inst.fit(X_train[:, 0])\n",
    "    le_delta = LabelEncoderExt()\n",
    "    le_delta.fit(X_train[:, 1])\n",
    "    print(\"----------------labels done----------------------\")\n",
    "    X_train[:, 0] = le_inst.transform(X_train[:, 0])\n",
    "    X_train[:, 1] = le_delta.transform(X_train[:, 1])\n",
    "    print(\"--------\")\n",
    "    \n",
    "    X_test[:, 0] = le_inst.transform(X_test[:, 0])\n",
    "    X_test[:, 1] = le_delta.transform(X_test[:, 1])\n",
    "    print(\"-------------------labels transformed---------------------\")\n",
    "    model = create_model()\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    filepath = '/nfs_home/nbhardwaj/model_weights/RD_Embedding/'+str(cset)+'.hdf5'\n",
    "    # checkpointer = ModelCheckpoint(filepath, monitor = 'val_acc', save_best_only = True, mode = 'max', verbose = 1)\n",
    "    model.fit([X_train[:, 0], X_train[:, 1], X_train[:, 2]], to_categorical(y_train), epochs = 2)\n",
    "    print(\"------------training done------------\")\n",
    "    model.save_weights(filepath)\n",
    "    t_ac = model.evaluate([X_test[:, 0], X_test[:, 1], X_test[:, 2]], to_categorical(y_test))[1]\n",
    "    test_acc.append(t_ac)\n",
    "    train_acc.append(model.evaluate([X_train[:, 0], X_train[:, 1], X_train[:, 2]], to_categorical(y_train))[1])\n",
    "    inst_vocab.append(len(le_inst.classes_))\n",
    "    delta_vocab.append(len(le_delta.classes_))\n",
    "    print(\"--------------done processing for set---------->\", cset, '|| accuracy||', t_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------split done---------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LabelEncoderExt at 0x7f0e5c0cce10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------labels done----------------------\n",
      "--------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------labels transformed---------------------\n",
      "WARNING:tensorflow:From /nfs_home/nbhardwaj/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/2\n",
      "178847/178847 [==============================] - 14s 80us/step - loss: 0.3742 - accuracy: 0.8792\n",
      "Epoch 2/2\n",
      "178847/178847 [==============================] - 14s 78us/step - loss: 0.3279 - accuracy: 0.8919\n",
      "------------training done------------\n",
      "88089/88089 [==============================] - 3s 34us/step\n",
      "178847/178847 [==============================] - 6s 34us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'delta_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b35011eaee2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minst_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle_inst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdelta_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------done processing for set---------->\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'|| accuracy||'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_ac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'delta_vocab' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ICount</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Data</th>\n",
       "      <th>Mode</th>\n",
       "      <th>set</th>\n",
       "      <th>rd</th>\n",
       "      <th>delta</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>638313090423</td>\n",
       "      <td>4722720</td>\n",
       "      <td>2198847782096</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>638313090428</td>\n",
       "      <td>4722742</td>\n",
       "      <td>3646153</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.198844e+12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>638313090748</td>\n",
       "      <td>4722720</td>\n",
       "      <td>2198847782096</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.198844e+12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>638313090753</td>\n",
       "      <td>4722742</td>\n",
       "      <td>3646153</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.198844e+12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>638313091064</td>\n",
       "      <td>4722720</td>\n",
       "      <td>2198847782096</td>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.198844e+12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ICount  Instruction           Data Mode  set  rd         delta  label\n",
       "0  638313090423      4722720  2198847782096    R    0   2           NaN      2\n",
       "1  638313090428      4722742        3646153    R    0   2 -2.198844e+12      2\n",
       "2  638313090748      4722720  2198847782096    R    0   2  2.198844e+12      2\n",
       "3  638313090753      4722742        3646153    R    0   2 -2.198844e+12      2\n",
       "4  638313091064      4722720  2198847782096    R    0   2  2.198844e+12      2"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.head()\n",
    "\n",
    "# # Read -> 1 ; Write -> -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(np.min(df.Instruction.values), np.max(df.Instruction.values))\n",
    "\n",
    "# print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# print(len(np.unique(X_train[:, 0])), len(np.unique(X[:, 0])), len(np.unique(X_train[:, 1])), len(np.unique(X[:, 1])))\n",
    "\n",
    "\n",
    "\n",
    "# le_inst = LabelEncoderExt()\n",
    "# le_inst.fit(X_train[:, 0])\n",
    "# le_delta = LabelEncoderExt()\n",
    "# le_delta.fit(X_train[:, 1])\n",
    "\n",
    "# X_train[:, 0] = le_inst.transform(X_train[:, 0])\n",
    "# X_train[:, 1] = le_delta.transform(X_train[:, 1])\n",
    "\n",
    "# X_test[:, 0] = le_inst.transform(X_test[:, 0])\n",
    "# X_test[:, 1] = le_delta.transform(X_test[:, 1])\n",
    "\n",
    "# print(len(le_inst.classes_), len(le_delta.classes_))\n",
    "\n",
    "# def create_model(embed_size = 10):\n",
    "#     inp1 = Input(shape = (1,))\n",
    "#     inp2 = Input(shape = (1,))\n",
    "#     inp3 = Input(shape = (1,))\n",
    "\n",
    "#     embed1 = Embedding(len(le_inst.classes_), embed_size, input_length = 1)(inp1)\n",
    "#     embed2 = Embedding(len(le_delta.classes_), embed_size, input_length = 1)(inp2)\n",
    "    \n",
    "#     merged_inp = keras.layers.concatenate([embed1, embed2], axis = 1)\n",
    "#     merged_inp = Flatten()(merged_inp)\n",
    "#     merged_inp = keras.layers.concatenate([merged_inp, inp3])\n",
    "    \n",
    "#     out = Dense(200, activation = 'relu')(merged_inp)\n",
    "#     out = Dense(8, activation = 'softmax')(out)\n",
    "    \n",
    "#     model = Model([inp1, inp2, inp3], out)\n",
    "#     return model\n",
    "    \n",
    "\n",
    "# model = create_model()\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# plot_model(model)\n",
    "\n",
    "# model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# filepath = '/nfs_home/nbhardwaj/model_weights/RD_Embedding/'+str(cset)+'.hdf5'\n",
    "\n",
    "# # checkpointer = ModelCheckpoint(filepath, monitor = 'val_acc', save_best_only = True, mode = 'max', verbose = 1)\n",
    "# model.fit([X_train[:, 0], X_train[:, 1], X_train[:, 2]], to_categorical(y_train), epochs = 2)\n",
    "\n",
    "\n",
    "# model.save_weights(filepath)\n",
    "\n",
    "# acc_test = model.evaluate([X_test[:, 0], X_test[:, 1], X_test[:, 2]], to_categorical(y_test))\n",
    "\n",
    "# print(acc_test[0])\n",
    "# print(model.metrics_names)\n",
    "\n",
    "# len(le_inst.classes_)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
